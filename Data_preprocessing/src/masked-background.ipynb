{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac079124-75f9-44cb-a69b-1d82f4411475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import random\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kamalkraj/BioELECTRA-PICO\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"kamalkraj/BioELECTRA-PICO\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "data = pd.read_csv(\"/home/st122802/work/Thesis/Untitled_Folder/cleaned_train-targets.csv\")\n",
    "\n",
    "def preprocess_background(background):\n",
    "    background = background.replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\").replace(\"\\n\", \" \")\n",
    "    inputs = tokenizer.encode(background, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    outputs = model(inputs).logits\n",
    "    predictions = outputs.argmax(dim=2)[0]\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "    \n",
    "    masked_tokens = []\n",
    "    for token, prediction in zip(tokens, predictions):\n",
    "        label = model.config.id2label[prediction.item()]\n",
    "\n",
    "        if label != 'O' and random.random() < 0.45:\n",
    "            token = tokenizer.mask_token\n",
    "            \n",
    "        if token not in ['[CLS]', '[SEP]']:\n",
    "            masked_tokens.append(token)\n",
    "\n",
    "    return \" \".join(masked_tokens)\n",
    "\n",
    "data['Masked Background'] = data['Background'].apply(preprocess_background)\n",
    "\n",
    "data.to_csv(\"/home/st122802/work/Thesis/Untitled_Folder/Final training sample/masked-background_PICO_0.45.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93f1675-d0c7-4aac-b601-6a91b2fc10f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "661ad5c6-e6ea-4db2-9960-81b9ec48022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "input_file_path = '/home/st122802/work/Thesis/Untitled Folder/train-targets.csv'\n",
    "\n",
    "data = pd.read_csv(input_file_path)\n",
    "\n",
    "\n",
    "mask_ratios = [0.15, 0.30, 0.45]\n",
    "\n",
    "for ratio in mask_ratios:\n",
    "    output_file_path = f'masked_background_random_{int(ratio)}.csv' \n",
    "    \n",
    "    masked_data = data.copy() \n",
    "    \n",
    "    for index, row in masked_data.iterrows():\n",
    "        background_text = row['Background']\n",
    "        words = background_text.split()\n",
    "        num_words_to_mask = max(1, int(len(words) * ratio))\n",
    "        masked_indices = random.sample(range(len(words)), num_words_to_mask)\n",
    "\n",
    "        for idx in masked_indices:\n",
    "            words[idx] = '[MASK]'\n",
    "\n",
    "        masked_text = ' '.join(words)\n",
    "        masked_data.at[index, 'Background'] = masked_text\n",
    "\n",
    "  \n",
    "    masked_data.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199a553-49b0-43f7-9e25-81a384667af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
